Problem:
Given target (true distribution) function of ![f(x)=y^2](http://mathurl.com/yarfcahw.png), with x distributed in ![-1,1](http://mathurl.com/ydbxaw6j.png), our dataset D is two points sampled from the target function, taking the form: ![set](http://mathurl.com/yaxpgzly.png).

We decide to use a linear learning algorithms of form ![hyp](http://mathurl.com/yap2towy.png). Find test performance (![eout](http://mathurl.com/yacz3c8s.png))
