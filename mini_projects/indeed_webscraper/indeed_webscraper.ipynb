{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea; treat each tag ex) <p> or </p> as a single character for the NN to learn\n",
    "# idea: replace entire \"<ul> <\\ul>\" with a single \"^\". Maybe the RNN can learn that behavior\n",
    "# Later, in generated text, replace ^s with lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indeed Webscraper\n",
    "This webscraper will be [indeed.com](indeed.com) specific; if more data is needed, we'll move to monster.com and other sites afterwards.\n",
    "\n",
    "Inputs: job titles to scrape on indeed\n",
    "Outputs: data that's ready to be trained/tested on\n",
    "\n",
    "### Plan:\n",
    "1. Get entire relevant job info and save first. We'll parse later.\n",
    "    * Get \"Machine Learning Engineer\", \"Data Scientist\", and \"Business Analyst\".\n",
    "    * Header\n",
    "        * Job title\n",
    "        * Company\n",
    "        * Location (City, State, Country)\n",
    "        * Salary\n",
    "    * Body\n",
    "        * Entire body, raw HTML\n",
    "2. Parse info (gotta think)\n",
    "    * Need to subdivide\n",
    "    * Bullet points are stored in -, html list, \\*, or sometimes just breaks.\n",
    "3. Use a markov generator on title dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "from datetime import datetime\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.indeed.com/jobs?q=\"\n",
    "VIEW_JOB = \"https://www.indeed.com/viewjob?jk=\"\n",
    "SEARCH_LOC = \"&l=united+states\" # can change this\n",
    "QUERY_PREFIX = \"&start=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seen = set()\n",
    "# TODO: Change this back later\n",
    "seen = posts\n",
    "\n",
    "def get_posts(query_job, num_posts):\n",
    "    \"\"\" Scrapes job data and returns list of dics\n",
    "    \n",
    "    Return dic:\n",
    "        {job_title: str\n",
    "         company: str\n",
    "         location: str\n",
    "         salary: str\n",
    "         description: str (raw html)}\n",
    "         \n",
    "    :param query_job: title of job to search (with \"+\"s instead of spaces)\n",
    "           num_posts: the number of job posts to scrape\n",
    "    :return list of dictionaries containing job info\n",
    "    \"\"\"\n",
    "    \n",
    "    jobs = []\n",
    "    req = 1\n",
    "    num_scraped = 0\n",
    "    dud_counter = 0\n",
    "\n",
    "    while num_scraped < num_posts:\n",
    "        # get search results\n",
    "        page = r.get(URL + query_job + SEARCH_LOC + QUERY_PREFIX + str(req * 10))\n",
    "        soup = bs(page.text, \"lxml\")\n",
    "        \n",
    "        req += 1\n",
    "\n",
    "        for item in soup.find_all(\"div\", {\"class\": \"unifiedRow\"}):\n",
    "            # break if you got enough posts\n",
    "            \n",
    "            if dud_counter > 30:\n",
    "                req += 20\n",
    "            \n",
    "            if num_scraped >= num_posts or dud_counter > 100:\n",
    "                return jobs\n",
    "            \n",
    "            if num_scraped%500 == 0 and num_scraped > 0 and dud_counter == 0:\n",
    "                print(\"Processed \", num_scraped, \" of \", query_job)\n",
    "            if dud_counter > 0 and dud_counter==50:\n",
    "                print(\"Num duds for\", query_job, \": 50\")\n",
    "\n",
    "            # enter subpage\n",
    "            job_id = item[\"data-jk\"]\n",
    "            post_raw = r.get(VIEW_JOB + job_id)\n",
    "            post = bs(post_raw.text, \"lxml\")\n",
    "            head = post.find(\"div\", class_=\"jobsearch-DesktopStickyContainer\")\n",
    "\n",
    "            # extract data\n",
    "            if head is None:\n",
    "                continue\n",
    "            title = head.find(\"h3\").text\n",
    "            company = head.find(\"a\").text if head.find(\"a\") else None\n",
    "            \n",
    "            # skip if it's the same company/job posted\n",
    "            if (title, company) in seen:\n",
    "                dud_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                seen.add((title, company))\n",
    "            \n",
    "            try:\n",
    "                location = (\n",
    "                    head.find(\"div\", class_=\"jobsearch-InlineCompanyRating\")\n",
    "                    .find_all(\"div\")[-1]\n",
    "                    .text\n",
    "                )\n",
    "            except:\n",
    "                location = None\n",
    "\n",
    "            salary = (\n",
    "                head.find(\"div\", class_=\"jobsearch-JobMetadataHeader-item\").text\n",
    "                if head.find(\"div\", class_=\"jobsearch-JobMetadataHeader-item\")\n",
    "                else None\n",
    "            )\n",
    "            description = post.find(\"div\", class_=\"jobsearch-jobDescriptionText\")\n",
    "            \n",
    "            jobs.append(\n",
    "                {\n",
    "                    \"title\": str(title),\n",
    "                    \"company\": str(company),\n",
    "                    \"location\": str(location),\n",
    "                    \"salary\": str(salary),\n",
    "                    \"description\": str(description),\n",
    "                }\n",
    "            )\n",
    "            # reset dud counter and increase num_scraped\n",
    "            dud_counter = 0\n",
    "            num_scraped += 1\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  500  of  data+analyst\n",
      "Num duds for data+analyst : 50\n",
      "Num duds for data+analyst : 50\n",
      "Num duds for data+analyst : 50\n",
      "Num duds for data+analyst : 50\n",
      "Num duds for data+analyst : 50\n",
      "Num duds for data+analyst : 50\n",
      "processed: data+analyst\n",
      "Processed  500  of  business+analyst\n",
      "Num duds for business+analyst : 50\n",
      "processed: business+analyst\n",
      "Processed  500  of  ai+research+scientist\n",
      "Num duds for ai+research+scientist : 50\n",
      "Num duds for ai+research+scientist : 50\n",
      "Num duds for ai+research+scientist : 50\n",
      "Num duds for ai+research+scientist : 50\n",
      "Num duds for ai+research+scientist : 50\n",
      "Num duds for ai+research+scientist : 50\n",
      "Num duds for ai+research+scientist : 50\n",
      "processed: ai+research+scientist\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# job titles to mine on\n",
    "titles = [\n",
    "#     \"machine+learning+engineer\",\n",
    "#           \"data+scientist\",\n",
    "#           \"big+data\",\n",
    "#           \"data+engineer\",\n",
    "          \"data+analyst\",\n",
    "          \"business+analyst\",\n",
    "          \"ai+research+scientist\"]\n",
    "\n",
    "for title in titles:\n",
    "    posts = get_posts(title, 2000)\n",
    "    name = title + \"+\" + str(datetime.utcnow().strftime('%m-%d')) + \".json\"\n",
    "    utils.save_json(posts , name)\n",
    "    print(\"processed: \" + title)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2499"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to json, so I don't have to repeat mining\n",
    "# utils.save_json(ml_posts, ml_eng + \"+5000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport json\n",
    "data_scientist = utils.load_json(\"data+scientist+11-09.json\")\n",
    "data_engineer = utils.load_json(\"data+engineer+11-09.json\")\n",
    "big_data = utils.load_json(\"big+data+11-09.json\")\n",
    "ml_eng = utils.load_json(\"machine+learning+engineer+11-09.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = set()\n",
    "for job in [data_scientist, data_engineer, big_data, ml_eng]:\n",
    "    for post in job:\n",
    "        posts.add((post['title'],post['company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(jobs):\n",
    "    \"\"\" Split the job descriptions into respective datasets\n",
    "    \n",
    "    :param jobs: the output from get_posts\n",
    "    :return body: list of body text strings\n",
    "            bullets: list of individual bullet points from each post\n",
    "            titles: list of paragraph/list titles\n",
    "    \"\"\"\n",
    "    \n",
    "    TAGS_TO_REMOVE = [\"<b>\", \"<div>\", \"</div>\", \"<br>\", \"<i>\"]\n",
    "    \n",
    "    body = []\n",
    "    bullets = []\n",
    "    titles = []\n",
    "    for post in jobs:\n",
    "        # first remove the div tags\n",
    "        text = post[\"description\"][66:]\n",
    "        text = text[:-6]\n",
    "        #     print(text)\n",
    "\n",
    "        # get titles and paragraphs\n",
    "        for paragraph in re.findall(r\"<p>\\s*(.*?)\\s*</p>\", text):\n",
    "            if \"<br/>\" in paragraph:\n",
    "                title_and_body = paragraph.split(\"<br/>\")\n",
    "                titles.append(title_and_body[0])\n",
    "                body.extend(title_and_body[1:])\n",
    "                continue\n",
    "            if \"·\" in paragraph:\n",
    "                bullets.append(paragraph[1:])\n",
    "                continue\n",
    "            if len(paragraph) < 60:\n",
    "                if \":\" in paragraph:\n",
    "                    if paragraph[-1] == \":\" or paragraph[-4:] == \"</b>\":\n",
    "                        titles.append(paragraph)\n",
    "                        continue\n",
    "                if \"<b>\" in paragraph:\n",
    "                    titles.extend(re.findall(r\"<b>\\s*(.*?)\\s*</b>\", paragraph))\n",
    "                    continue\n",
    "            body.append(paragraph)\n",
    "\n",
    "        # get all bullet lists if any\n",
    "        if \"<ul>\" in text:\n",
    "            for item in re.findall(r\"<ul>\\s*(.*?)\\s*</ul>\", text):\n",
    "                bullets.extend(re.findall(r\"<li>\\s*(.*?)\\s*</li>\", item))\n",
    "        else:\n",
    "            re.sub('<[^<]+?>', '', text)\n",
    "            body.append(text)\n",
    "        \n",
    "    return body, bullets, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_list, bullet_list, title_list = split(ml_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal of a research engineer at scale is to bring techniques in the fields of computer vision, deep learning and deep reinforcement learning, or natural language processing into a production environment to improve scale.ai ’s products and customer experience. Our research engineers take advantage of our unique access to massive datasets to deliver improvements to our customers.\n",
      "<br/>We are building a large hybrid human-machine system in service of ML pipelines for dozens of industry-leading customers. We currently complete millions of tasks a month, and will grow to complete billions of tasks monthly. As a Research Engineer, you will:\n",
      "<br/>Take state of the art models developed internally and from the community, use them in production to solve problems for our customers and taskers.\n",
      "<br/>Take models currently in production, identify areas for improvement, improve them using retraining and hyperparemeter searches, then deploy without regressing on core model characteristics\n",
      "<br/>Work with product and research teams to identify opportunities for improvement in our current product line and for enabling upcoming product lines\n",
      "<br/>Work with massive datasets to develop both generic models as well as fine tune models for specific products\n",
      "<br/>Build the scalable ML platform to automate our ML services\n",
      "<br/>Be a representative for how to apply machine learning and related techniques throughout the engineering and product organization\n",
      "<br/>Be able, and willing, to multi-task and learn new technologies quickly Requirements:\n",
      "<br/>Degree in computer science, or related field\n",
      "<br/>Experience using computer vision, deep learning and deep reinforcement Learning, or natural language processing in a production environment\n",
      "<br/>Solid background in algorithms, data structures, and object-oriented programming\n",
      "<br/>Strong programing skills in Python or Javascript, experience in Tensorflow or PyTorch Nice to Haves:\n",
      "<br/>Graduate degree in Computer Science, Machine Learning or Artificial Intelligence specialization\n",
      "<br/>Experience working with cloud technology stack (eg. AWS or GCP) and developing machine learning models in a cloud environment\n",
      "<br/><br/>\n",
      "<b>About Us:</b> At Scale, our mission is to accelerate the development of Machine Learning and AI applications across multiple markets. Our first product is a suite of APIs that allow AI teams to generate high-quality ground truth data. Our customers include Alphabet (Google), Zoox, Lyft, Pinterest, Airbnb, nuTonomy, and many more\n",
      "<br/><b>Additional Information:</b> Scale is an equal opportunity employer. We aim for every person at Scale to feel like they matter, belong, and can be their authentic selves so they can do their best work. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n"
     ]
    }
   ],
   "source": [
    "# # get distribution of paragraph lengths (for generator later)\n",
    "# body_lens = [len(s) for s in body_list if len(s) > 1]\n",
    "# # print(max(body_lens))\n",
    "# longest_s = \"\"\n",
    "# for b in body_list:\n",
    "#     if len(b) > len(longest_s):\n",
    "#         longest_s = b\n",
    "# print(longest_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_list_as_corpus(body_list, '5000_body_list.txt')\n",
    "utils.save_list_as_corpus(bullet_list, '5000_bullet_list.txt')\n",
    "utils.save_list_as_corpus(title_list, '5000_title_list.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
